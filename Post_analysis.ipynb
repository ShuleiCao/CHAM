{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de65ce88-425f-4e2b-ae80-09a6edeac6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "from astropy.table import Table, Column\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def consolidate_hdf5_files_with_centrals(input_folder, output_file, redmapper_centrals_data):\n",
    "    # Open the main consolidated HDF5 file\n",
    "    with h5py.File(output_file, 'w') as main_file:\n",
    "        \n",
    "        # Iterate over individual .h5 files in the input folder\n",
    "        for filename in os.listdir(input_folder):\n",
    "            if filename.endswith('.h5'):\n",
    "                cluster_id = filename.split('.')[0]  # Extract cluster ID from filename\n",
    "                \n",
    "                # Open individual HDF5 file and read its content\n",
    "                with h5py.File(os.path.join(input_folder, filename), 'r') as cluster_file:\n",
    "                    # Create a group in the main file for this cluster\n",
    "                    cluster_group = main_file.create_group(cluster_id)\n",
    "                    \n",
    "                    # Add 'centrals' data for the cluster\n",
    "                    centrals_group = cluster_group.create_group('centrals')\n",
    "                    mask = redmapper_centrals_data['mem_match_id'] == int(cluster_id)\n",
    "                    for col_name in redmapper_centrals_data.colnames:\n",
    "                        centrals_group.create_dataset(col_name, data=redmapper_centrals_data[col_name][mask])\n",
    "                    \n",
    "                    # Copy data from the individual file to the main file\n",
    "                    for category in cluster_file.keys():\n",
    "                        if category != 'centrals':  # We've already handled 'centrals'\n",
    "                            category_group = cluster_group.create_group(category)\n",
    "                            for halo_id in cluster_file[category].keys():\n",
    "                                halo_group = category_group.create_group(halo_id)\n",
    "                                for dataset in cluster_file[category][halo_id].keys():\n",
    "                                    data = cluster_file[category][halo_id][dataset][:]\n",
    "                                    halo_group.create_dataset(dataset, data=data)\n",
    "                                for attr in cluster_file[category][halo_id].attrs.keys():\n",
    "                                    halo_group.attrs[attr] = cluster_file[category][halo_id].attrs[attr]\n",
    "\n",
    "    print(\"Consolidation complete!\")\n",
    "\n",
    "base_path = '/lustre/work/client/users/shuleic/Cardinalv3/'\n",
    "# suffix = '_lgt20'\n",
    "suffix = '_lgt05'\n",
    "redmapper_member_data_old = Table.read(os.path.join(base_path, f'chunhao-redmapper{suffix}_mem_data_all.fits'))\n",
    "redmapper_member_data = Table.read(os.path.join(base_path, f'chunhao-redmapper{suffix}_mem_data_all_new.fits'))\n",
    "redmapper_members = Table.read(os.path.join(base_path, 'redmapper_v4_v8_v51_y6_v7/run/', f'Cardinal-3Y6a_v2.0_run_run_redmapper_v0.8.1{suffix}_vl02_catalog_members.fit'))\n",
    "redmapper_data = Table.read(os.path.join(base_path, 'redmapper_v4_v8_v51_y6_v7/run/', f'Cardinal-3Y6a_v2.0_run_run_redmapper_v0.8.1{suffix}_vl02_catalog.fit'))\n",
    "\n",
    "output_file = os.path.join(base_path, f'sorted-chunhao-redmapper{suffix}_data_new.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bc58ad",
   "metadata": {},
   "source": [
    "# Get redMaPPer central members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a41ea249-2ca8-45c3-94a3-4497508d182e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from astropy.table import Table\n",
    "\n",
    "# Convert redmapper_member_data to a pandas DataFrame\n",
    "redmapper_member_df = redmapper_member_data.to_pandas()\n",
    "\n",
    "# Create a list to hold expanded data for redmapper_data with an additional index\n",
    "redmapper_data_expanded = []\n",
    "\n",
    "for mem_id, ids in zip(redmapper_data['mem_match_id'], redmapper_data['id_cent']):\n",
    "    for id_cent in ids:\n",
    "        redmapper_data_expanded.append({'mem_match_id': mem_id, 'id_cent': id_cent})\n",
    "\n",
    "# Create a DataFrame for the expanded redmapper_data\n",
    "redmapper_data_df = pd.DataFrame(redmapper_data_expanded)\n",
    "\n",
    "# Add an index to preserve the original order\n",
    "redmapper_data_df['original_index'] = redmapper_data_df.index\n",
    "\n",
    "# Merge the DataFrames on mem_match_id and id_cent\n",
    "central_members_df = pd.merge(redmapper_member_df, redmapper_data_df, \n",
    "                               left_on=['mem_match_id', 'coadd_object_id'], \n",
    "                               right_on=['mem_match_id', 'id_cent'], \n",
    "                               how='inner')\n",
    "\n",
    "# Sort by original index to preserve the order of id_cent from each mem_match_id\n",
    "central_members_df = central_members_df.sort_values(by='original_index')\n",
    "\n",
    "# Convert back to an Astropy Table\n",
    "central_members = Table.from_pandas(central_members_df)\n",
    "redmapper_centrals_path = os.path.join(base_path, f'chunhao-redmapper{suffix}_centrals_data_all.fits')\n",
    "\n",
    "central_members.write(redmapper_centrals_path, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3742c151",
   "metadata": {},
   "source": [
    "# Consolidate Matched data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0301f6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "redmapper_centrals_path = os.path.join(base_path, f'chunhao-redmapper{suffix}_centrals_data_all.fits')\n",
    "central_members = Table.read(redmapper_centrals_path)\n",
    "consolidate_hdf5_files_with_centrals(os.path.join(base_path, f'cluster{suffix}_data'), output_file, central_members)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05f6377",
   "metadata": {},
   "source": [
    "# Check for missed members in which clusters (if there's a bad run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a13ea85-62b2-46c4-9ae5-cd0b1a87d6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatched cluster IDs: [67385, 163926, 411790, 416391, 453733, 461523, 479073, 801366, 820198]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from astropy.table import Table\n",
    "\n",
    "# Assuming redmapper_member_data_old and redmapper_member_data are Astropy Tables\n",
    "old_cids = redmapper_member_data_old['mem_match_id'].data.astype(np.int32)\n",
    "new_cids = redmapper_member_data['mem_match_id'].data.astype(np.int32)\n",
    "\n",
    "# Count occurrences of mem_match_id in both arrays\n",
    "old_counts = np.unique(old_cids, return_counts=True)\n",
    "new_counts = np.unique(new_cids, return_counts=True)\n",
    "\n",
    "# Create DataFrames from counts\n",
    "old_counts_df = pd.DataFrame({'mem_match_id': old_counts[0], 'old_count': old_counts[1]})\n",
    "new_counts_df = pd.DataFrame({'mem_match_id': new_counts[0], 'new_count': new_counts[1]})\n",
    "\n",
    "# Merge the counts on mem_match_id\n",
    "merged_counts = pd.merge(old_counts_df, new_counts_df, on='mem_match_id', how='outer').fillna(0)\n",
    "\n",
    "# Find mismatches\n",
    "mismatched_ids = merged_counts[merged_counts['old_count'] != merged_counts['new_count']]\n",
    "\n",
    "# Output the mismatched cluster IDs\n",
    "print(\"Mismatched cluster IDs:\", mismatched_ids['mem_match_id'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2c5438b-bee1-4044-8a7c-75625c283488",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(os.path.join(base_path, f'mismatched_clusters{suffix}.npz'),mismatched_ids=mismatched_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230544b9-a349-4ee7-ba2d-dfbc0881febf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2716f8-a0d1-4164-87f9-2760752a7e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.table import Table, Column\n",
    "from astropy.io import fits\n",
    "\n",
    "halo_paths = ['/Users/shuleicao/Documents/halo_data_all.fits','/Users/shuleicao/Documents/halo_data_all_new.fits']\n",
    "\n",
    "def get_name(file_path, names, mask=None, file_format='fits', key='gold'):\n",
    "    if file_format == 'fits':\n",
    "        table_data = Table.read(file_path)  # Directly read the FITS file into an Astropy Table\n",
    "        if mask is not None:\n",
    "            table_data = table_data[mask]  # Apply the mask directly to the table\n",
    "\n",
    "        data_names = {name: table_data[name] for name in names}  # Extract the relevant columns\n",
    "    elif file_format == 'hdf5':\n",
    "        with h5py.File(file_path, 'r') as f:\n",
    "            data_names = {name: f['catalog/'][key][name][:][mask] if mask is not None else f['catalog/'][key][name][:] for name in names}\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown file format: {file_format}\")\n",
    "    return data_names\n",
    "\n",
    "def get_data(file_path, mask=None, file_format='fits', key='gold'):\n",
    "    data_names = {}\n",
    "    \n",
    "    if file_format == 'fits':\n",
    "        with fits.open(file_path) as hdul:\n",
    "            data = hdul[1].data\n",
    "            for name in data.names:\n",
    "                data_names[name] = data[name][mask] if mask is not None else data[name]\n",
    "    elif file_format == 'hdf5':\n",
    "        with h5py.File(file_path, 'r') as f:\n",
    "            for name in f['catalog/'][key].keys():\n",
    "                data_names[name] = f['catalog/'][key][name][:][mask] if mask is not None else f['catalog/'][key][name][:]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown file format: {file_format}\")\n",
    "    \n",
    "    return data_names\n",
    "\n",
    "# halo_data = get_data(halo_paths[0], file_format='fits')\n",
    "halo_data_new = get_data(halo_paths[1], file_format='fits')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bc86e7-444d-4f48-8d5b-7ae61930508f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "def read_all_clusters_hdf5_data(file_path):\n",
    "    all_data = {}\n",
    "    \n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        # Iterate through all clusters\n",
    "        for cluster_id in f.keys():\n",
    "            cluster_group = f.get(cluster_id)\n",
    "            if cluster_group is None:\n",
    "                continue\n",
    "\n",
    "            cluster_data = {}\n",
    "            for category in cluster_group.keys():\n",
    "                if category != 'centrals':\n",
    "                    category_group = cluster_group.get(category)\n",
    "                    cluster_data[category] = []  \n",
    "                    for halo_id in category_group.keys():\n",
    "                        halo_group = category_group.get(halo_id)\n",
    "                        if halo_group is not None:\n",
    "                            halo_data = {}\n",
    "                            halo_data['haloid'] = int(halo_id)\n",
    "                            halo_data['m200'] = halo_group.attrs.get('m200', None)\n",
    "                            halo_data['n_members'] = halo_group.attrs.get('n_members', None)\n",
    "                            members_data = {}\n",
    "\n",
    "                            # Read members data\n",
    "                            for col_name in halo_group.keys():\n",
    "                                members_data[col_name] = np.array(halo_group[col_name])\n",
    "\n",
    "                            halo_data['members'] = members_data\n",
    "                            cluster_data[category].append(halo_data)\n",
    "                else:\n",
    "                    category_data = cluster_group.get(category)\n",
    "                    if category_data is not None:\n",
    "                        members_data = {}\n",
    "\n",
    "                        # Read members data\n",
    "                        for col_name in category_data.keys():\n",
    "                            members_data[col_name] = np.array(category_data[col_name])\n",
    "\n",
    "                        cluster_data[category] = members_data\n",
    "            \n",
    "            all_data[cluster_id] = cluster_data\n",
    "\n",
    "    return all_data\n",
    "complete_data = read_all_clusters_hdf5_data(os.path.join(base_path, f'sorted-chunhao-redmapper{suffix}_data_new.h5'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0df53e-397c-4d58-bd46-4bbd2208835d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_complete_data(complete_data, halo_data, redmapper_members):\n",
    "    processed_data = {}\n",
    "\n",
    "\n",
    "    for cluster_id, cluster_values in complete_data.items():\n",
    "        largest_halo_data = {}\n",
    "        central_matched_halo_data = {}\n",
    "\n",
    "        mask = redmapper_members['mem_match_id']==int(cluster_id)\n",
    "        masked_redmapper_members = redmapper_members[mask]\n",
    "        id_to_p = dict(zip(masked_redmapper_members['id'], masked_redmapper_members['p'] * masked_redmapper_members['pfree']))\n",
    "\n",
    "        # Calculate 'p_mem' for each halo and sort them\n",
    "#         for halo in cluster_values.get('associated', []):\n",
    "#             # Ensure 'coadd_object_id' is in the 'members' data\n",
    "#             if 'coadd_object_id' in halo['members']:\n",
    "#                 # Map each 'coadd_object_id' to its 'p' value, or 0 if not found\n",
    "#                 halo['members']['p_mem'] = np.array(\n",
    "#                     [id_to_p.get(coadd_id, 0) for coadd_id in halo['members']['coadd_object_id']]\n",
    "#                 )\n",
    "#                 # Calculate the sum of 'p_mem' for the current halo\n",
    "#                 halo['p_mem_sum'] = np.sum(halo['members']['p_mem'])\n",
    "#         sorted_halos = sorted(cluster_values.get('associated', []), key=lambda x: x.get('p_mem_sum', 0), reverse=True)\n",
    "\n",
    "        for halo in cluster_values.get('associated', []):\n",
    "            if 'coadd_object_id' in halo['members']:\n",
    "                halo['members']['p_mem'] = np.array([id_to_p[coadd_id] for coadd_id in halo['members']['coadd_object_id'] if coadd_id in id_to_p])\n",
    "                halo['p_mem_sum'] = np.sum(halo['members']['p_mem'])\n",
    "\n",
    "        sorted_halos = sorted(cluster_values.get('associated', []), key=lambda x: x.get('p_mem_sum'), reverse=True)\n",
    "\n",
    "#         if sorted_halos:\n",
    "#             largest_halo = sorted_halos[0]\n",
    "# #             print(f\"Debug: Cluster ID {cluster_id}, Largest Halo ID: {largest_halo['haloid']}\")\n",
    "\n",
    "#             largest_halo_data['haloid'] = largest_halo['haloid']\n",
    "# #             matching_indices = np.isin(largest_halo['members']['haloid'], largest_halo_data['haloid'])\n",
    "\n",
    "# #             largest_halo_data['m200'] =  np.unique(largest_halo['members']['m200'][matching_indices])\n",
    "#             largest_halo_data['m200'] = largest_halo['m200']\n",
    "#             largest_halo_data['members'] = largest_halo['members']\n",
    "\n",
    "#             # Check if the member haloids match\n",
    "#             if any(largest_halo_data['haloid'] != member_halo_id for member_halo_id in largest_halo['members']['haloid']):\n",
    "#                 print(f\"Warning: Mismatch in haloid for cluster ID {cluster_id}\")\n",
    "\n",
    "#             # Extract \"halo galaxies\" from halo_data for the largest halo ID\n",
    "#             halo_data_mask = (halo_data['haloid'] == largest_halo_data['haloid']) & (halo_data['m200'] == largest_halo_data['m200'])\n",
    "#             halo_data_halo_galaxies = {key: halo_data[key][halo_data_mask] for key in halo_data.keys()}\n",
    "#             largest_halo_data['halo_galaxies'] = halo_data_halo_galaxies\n",
    "#         else:\n",
    "#             # Handle the case where there are no associated halos\n",
    "#             print(f\"No associated halos for cluster ID {cluster_id}\")\n",
    "#             continue  # or set largest_halo_data to a default value\n",
    "\n",
    "        if sorted_halos:\n",
    "            halo_index = 0\n",
    "            found_matching_halo = False\n",
    "\n",
    "            while halo_index < len(sorted_halos) and not found_matching_halo:\n",
    "                halo = sorted_halos[halo_index]\n",
    "                halo_mask = (halo_data['haloid'] == halo['haloid']) & (halo_data['m200'] == halo['m200'])\n",
    "\n",
    "                if np.any(halo_mask):\n",
    "                    largest_halo_data['haloid'] = halo['haloid']\n",
    "                    largest_halo_data['m200'] = halo['m200']\n",
    "                    largest_halo_data['members'] = halo['members']\n",
    "\n",
    "                    halo_data_halo_galaxies = {key: halo_data[key][halo_mask] for key in halo_data.keys()}\n",
    "                    largest_halo_data['halo_galaxies'] = halo_data_halo_galaxies\n",
    "\n",
    "                    found_matching_halo = True\n",
    "\n",
    "                    # Check if the member haloids match\n",
    "                    if any(largest_halo_data['haloid'] != member_halo_id for member_halo_id in halo['members']['haloid']):\n",
    "                        print(f\"Warning: Mismatch in haloid for cluster ID {cluster_id}\")\n",
    "                else:\n",
    "                    halo_index += 1\n",
    "\n",
    "            if not found_matching_halo:\n",
    "                print(f\"No matching halos found for cluster ID {cluster_id}\")\n",
    "        else:\n",
    "            print(f\"No associated halos for cluster ID {cluster_id}\")\n",
    "\n",
    "        # Match the haloid with centrals and save\n",
    "        matched_central_data = None\n",
    "        centrals_data = cluster_values.get('centrals', {})\n",
    "        for i, central_haloid in enumerate(centrals_data.get('haloid', [])):\n",
    "            if central_haloid in [halo['haloid'] for halo in sorted_halos] and centrals_data.get('rhalo', [])[i] == 0:\n",
    "                matched_central_data = {col_name: centrals_data[col_name][i] for col_name in centrals_data.keys()}\n",
    "                central_matched_halo_data['haloid'] = central_haloid\n",
    "                central_matched_halo_data['members'] = next(halo for halo in sorted_halos if halo['haloid'] == central_haloid)['members']\n",
    "                \n",
    "                # Extract \"halo galaxies\" from halo_data for the central matched halo ID\n",
    "                halo_data_mask = halo_data['haloid'] == central_matched_halo_data['haloid']\n",
    "                halo_data_halo_galaxies = {key: halo_data[key][halo_data_mask] for key in halo_data.keys()}\n",
    "                central_matched_halo_data['halo_galaxies'] = halo_data_halo_galaxies\n",
    "                break\n",
    "\n",
    "        # Fallback mechanism\n",
    "        if matched_central_data is None and 'haloid' in centrals_data:\n",
    "            matched_central_data = {}\n",
    "            has_empty_arrays = False\n",
    "            for col_name in centrals_data.keys():\n",
    "                if len(centrals_data[col_name]) == 0:\n",
    "                    if not has_empty_arrays:\n",
    "                        print(f\"Cluster with ID {cluster_id} has empty arrays in centrals data.\")\n",
    "                        has_empty_arrays = True\n",
    "                    matched_central_data[col_name] = None\n",
    "                else:\n",
    "                    matched_central_data[col_name] = centrals_data[col_name][0]\n",
    "\n",
    "\n",
    "        # Store in the processed data structure\n",
    "        processed_data[cluster_id] = {\n",
    "            'largest_halo': largest_halo_data,\n",
    "            'central_matched_halo': central_matched_halo_data,\n",
    "            'centrals': matched_central_data\n",
    "        }\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "processed_data = process_complete_data(complete_data, halo_data_new, redmapper_members)\n",
    "# processed_data = process_complete_data(complete_data, masked_halo, redmapper_members)\n",
    "\n",
    "def save_processed_data_to_hdf5(processed_data, output_path):\n",
    "    with h5py.File(output_path, 'w') as f:\n",
    "        for cluster_id, values in processed_data.items():\n",
    "            cluster_group = f.create_group(cluster_id)\n",
    "\n",
    "            # Function to save halo data\n",
    "            def save_halo_data(group_name, halo_data):\n",
    "                if 'halo_galaxies' not in halo_data:\n",
    "                    return  # Skip saving if no halo_galaxies data is present\n",
    "\n",
    "                halo_group = cluster_group.create_group(group_name)\n",
    "\n",
    "                # Save halo_galaxies data\n",
    "                halo_galaxies_group = halo_group.create_group('halo_galaxies')\n",
    "                for col_name, col_data in halo_data['halo_galaxies'].items():\n",
    "                    halo_galaxies_group.create_dataset(col_name, data=col_data)\n",
    "\n",
    "                # Save members data\n",
    "                members_group = halo_group.create_group('members')\n",
    "                for col_name, col_data in halo_data['members'].items():\n",
    "                    members_group.create_dataset(col_name, data=col_data)\n",
    "\n",
    "            # Save data for largest_halo and central_matched_halo\n",
    "            save_halo_data('largest_halo', values['largest_halo'])\n",
    "            save_halo_data('central_matched_halo', values['central_matched_halo'])\n",
    "\n",
    "            # Save centrals data\n",
    "            centrals_group = cluster_group.create_group('centrals')\n",
    "            for col_name, col_data in values['centrals'].items():\n",
    "                try:\n",
    "                    # Convert data to a numpy array for saving\n",
    "                    data_to_save = np.array([col_data])\n",
    "                    centrals_group.create_dataset(col_name, data=data_to_save)\n",
    "                except TypeError:\n",
    "                    # If the data type is unsupported, print out detailed information\n",
    "                    print(f\"Error with cluster_id: {cluster_id}, column: {col_name}, data: {col_data}\")\n",
    "                    # If the data is a string, encode it and save\n",
    "                    if isinstance(col_data, str):\n",
    "                        centrals_group.create_dataset(col_name, data=np.array([col_data.encode('utf-8')]), dtype=h5py.string_dtype(encoding='utf-8'))\n",
    "                    # If the data is None, we won't save it\n",
    "                    elif col_data is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        # For other unsupported data types, you can add handling here\n",
    "                        raise\n",
    "\n",
    "    print(\"Data saved successfully!\")\n",
    "\n",
    "# Save the processed data\n",
    "output_file_path = os.path.join(base_path, f'processed_sorted-chunhao-redmapper{suffix}_data_new_true_pmem.h5')\n",
    "save_processed_data_to_hdf5(processed_data, output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c51a23-48b8-4f9f-8db2-20488c133579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "def read_processed_data_from_hdf5(input_path):\n",
    "    processed_data = {}\n",
    "\n",
    "    with h5py.File(input_path, 'r') as f:\n",
    "        for cluster_id in f.keys():\n",
    "            cluster_group = f.get(cluster_id)\n",
    "            \n",
    "            largest_halo_data = {}\n",
    "            central_matched_halo_data = {}\n",
    "            centrals_data = {}\n",
    "\n",
    "            # Function to read halo data\n",
    "            def read_halo_data(group_name):\n",
    "                halo_data = {}\n",
    "                \n",
    "                halo_group = cluster_group.get(group_name)\n",
    "                if halo_group is None:\n",
    "                    return halo_data\n",
    "                \n",
    "                # Extract halo_galaxies data\n",
    "                if 'halo_galaxies' in halo_group:\n",
    "                    halo_galaxies_group = halo_group['halo_galaxies']\n",
    "                    halo_data['halo_galaxies'] = {col_name: np.array(halo_galaxies_group[col_name]) for col_name in halo_galaxies_group.keys()}\n",
    "                \n",
    "                # Extract members data\n",
    "                members_group = halo_group['members']\n",
    "                halo_data['members'] = {col_name: np.array(members_group[col_name]) for col_name in members_group.keys()}\n",
    " \n",
    "                # Extract haloid from the members data\n",
    "                halo_data['haloid'] = halo_data['members']['haloid'][0]\n",
    "                \n",
    "                return halo_data\n",
    "\n",
    "            # Read data for largest_halo and central_matched_halo\n",
    "            largest_halo_data = read_halo_data('largest_halo')\n",
    "            central_matched_halo_data = read_halo_data('central_matched_halo')\n",
    "\n",
    "            # Read centrals data\n",
    "            centrals_group = cluster_group.get('centrals')\n",
    "            for col_name in centrals_group.keys():\n",
    "                dataset = centrals_group[col_name]\n",
    "                if dataset.dtype.kind == 'S':  # check if it's a bytestring\n",
    "                    centrals_data[col_name] = dataset[0].decode('utf-8')\n",
    "                else:\n",
    "                    centrals_data[col_name] = dataset[0]\n",
    "\n",
    "            # Store in the processed data structure\n",
    "            processed_data[cluster_id] = {\n",
    "                'largest_halo': largest_halo_data,\n",
    "                'central_matched_halo': central_matched_halo_data,\n",
    "                'centrals': centrals_data\n",
    "            }\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "# Read the processed data\n",
    "input_file_path = os.path.join(base_path, f'processed_sorted-chunhao-redmapper{suffix}_data_new_true_pmem.h5')\n",
    "loaded_data = read_processed_data_from_hdf5(input_file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (redmapper_env)",
   "language": "python",
   "name": "redmapper_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
